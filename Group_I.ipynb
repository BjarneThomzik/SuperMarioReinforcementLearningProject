{"cells":[{"cell_type":"markdown","metadata":{"id":"XsmfgIagXlts"},"source":["# Super Mario Bros Gym\n","This is the example notebook for the seminar:\n","\n","392142 Applied Cognitive Computing: Advancing Reinforcement Learning Agents through Cognitive Mechanisms\n","\n","You can change any part of this colab for your group work. Simply go to File -> \"Save a Copy to Drive\" and start changing parts of the code.\n","\n","Please don't use any reinforcement learning libraries for your group work (Stable Baselines, Chainer RL, and so on). The idea is not to simply plug in the state of the art model, but learn what works and what doesn't work by modifying an algorithm.\n","\n","For example, for the group work you could:\n","- Change the reward model\n","- Change the input\n","- Change parts of PPO\n","- Change parts of the environment\n","- Implement a different Reinforcement Learning algorithm\n","- Try out non Reinforcement Learning algorithms (MDP, POMDP, Genetic Algorithms)\n","- Optimize part of the environment\n","- Try out ANY other idea you have that could help in getting mario as far as possible.\n"]},{"cell_type":"markdown","metadata":{"id":"ljVkIDl4YfNf"},"source":["# Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5582,"status":"ok","timestamp":1745690733774,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"},"user_tz":-120},"id":"GVM8WE3eXZY4","outputId":"6572c5e8-6984-410d-b014-8fecf548cd13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym==0.24.1 in /usr/local/lib/python3.11/dist-packages (0.24.1)\n","Requirement already satisfied: gym-super-mario-bros==7.4.0 in /usr/local/lib/python3.11/dist-packages (7.4.0)\n","Requirement already satisfied: nes_py==8.2.1 in /usr/local/lib/python3.11/dist-packages (8.2.1)\n","Requirement already satisfied: numpy==1.22.0 in /usr/local/lib/python3.11/dist-packages (1.22.0)\n","Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.11/dist-packages (2.2.0)\n","Requirement already satisfied: torchaudio==2.2.0 in /usr/local/lib/python3.11/dist-packages (2.2.0)\n","Requirement already satisfied: torchrl==0.3.0 in /usr/local/lib/python3.11/dist-packages (0.3.0)\n","Requirement already satisfied: torchvision==0.17.0 in /usr/local/lib/python3.11/dist-packages (0.17.0)\n","Requirement already satisfied: tqdm==4.66.5 in /usr/local/lib/python3.11/dist-packages (4.66.5)\n","Requirement already satisfied: matplotlib==3.5.2 in /usr/local/lib/python3.11/dist-packages (3.5.2)\n","Requirement already satisfied: scipy==1.11.1 in /usr/local/lib/python3.11/dist-packages (1.11.1)\n","Requirement already satisfied: mediapy in /usr/local/lib/python3.11/dist-packages (1.2.3)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.24.1) (3.1.1)\n","Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.24.1) (0.0.8)\n","Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from nes_py==8.2.1) (1.5.21)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (4.13.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchrl==0.3.0) (24.2)\n","Requirement already satisfied: tensordict>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from torchrl==0.3.0) (0.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (11.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.5.2) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.5.2) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.5.2) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.5.2) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.5.2) (2.8.2)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.5.82)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from mediapy) (7.34.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.17.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (75.2.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (4.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2025.1.31)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy) (0.2.13)\n"]}],"source":["! pip install gym==0.24.1 gym-super-mario-bros==7.4.0 nes_py==8.2.1 numpy==1.22.0 torch==2.2.0  torchaudio==2.2.0  torchrl==0.3.0  torchvision==0.17.0  tqdm==4.66.5 matplotlib==3.5.2 scipy==1.11.1 mediapy\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mwSex1aRZplg"},"source":["## Checking the gpu that is provided by google colab.\n","If you don't see a gpu listed, or an error is shown, please click on the small dropdown arrow on the top right corner and choose \"View Ressources\" from the appearing menu.\n","\n","From there you can click on \"Change runtime type\" on the bottom right.\n","\n","In the appearing window, you should be able to select a \"Hardware accelerator\", which you will have to set to \"GPU\""]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1745690826310,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"},"user_tz":-120},"id":"8oEVD5spYICr","outputId":"d801acb2-7903-477c-80f5-447eb40d777e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Apr 26 18:07:06 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["! nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"odqlWAkUanmT"},"source":["# PPO Algorithm\n","\n","Here we define the PPO Algorithm, that can be freely extended during the seminar."]},{"cell_type":"markdown","metadata":{"id":"Q_y-oupXa4eZ"},"source":["The imports used for the PPO algorithm"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZZJwF2ddafxt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745690839044,"user_tz":-120,"elapsed":2685,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"}},"outputId":"6ed138fc-594f-4086-877b-17f4bb1899a1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n","  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n","Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.distributions import MultivariateNormal\n","from torch.distributions import Categorical\n","from nes_py.wrappers import JoypadSpace\n","import gym_super_mario_bros\n","from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT\n","from datetime import datetime\n","import cv2\n","from tqdm import tqdm\n","import numpy as np\n","from gym.wrappers import GrayScaleObservation"]},{"cell_type":"markdown","metadata":{"id":"3gczdoo9a9mH"},"source":["Setting the device to gpu if available"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1745690848341,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"},"user_tz":-120},"id":"AYDdPY8tYY_e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0065c5a9-cd3e-4d0a-d336-43584be1fca4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device set to : Tesla T4\n"]}],"source":["# set device to cpu or cuda\n","device = torch.device('cpu')\n","if(torch.cuda.is_available()):\n","    device = torch.device('cuda:0')\n","    torch.cuda.empty_cache()\n","    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n","\n","else:\n","    print(\"Device set to : cpu\")\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"UzUoRPJzboly"},"source":["## ActorCritic\n","Here we define the actor critic agent that functions as our neural network.\n","To get a better understanding of the algorithm you can take a look at: https://theaisummer.com/Actor_critics/"]},{"cell_type":"markdown","source":["## RolloutBuffer"],"metadata":{"id":"_QKD_k1SmMwv"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"XclKPLRwe-jS","executionInfo":{"status":"ok","timestamp":1745690858109,"user_tz":-120,"elapsed":43,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"}}},"outputs":[],"source":["# The RolloutBuffer which keeps the training tuples.\n","class RolloutBuffer:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.is_terminals = []\n","\n","    # We clear the buffer after each training update\n","    def clear(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.is_terminals[:]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"7Wo4XyWjbPve","executionInfo":{"status":"ok","timestamp":1745690860081,"user_tz":-120,"elapsed":10,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"}}},"outputs":[],"source":["class ActorCritic(nn.Module):\n","    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n","        super(ActorCritic, self).__init__()\n","        # We define if the action space is continuous or a value choosen from a set of possible actions\n","        self.has_continuous_action_space = has_continuous_action_space\n","\n","\n","        if has_continuous_action_space:\n","            self.action_dim = action_dim\n","            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n","        # defining the actor\n","        if has_continuous_action_space :\n","            self.actor = nn.Sequential(\n","                            nn.Linear(state_dim, 64),\n","                            nn.Tanh(),\n","                            nn.Linear(64, 64),\n","                            nn.Tanh(),\n","                            nn.Linear(64, action_dim),\n","                        )\n","        else:\n","            self.actor = nn.Sequential(\n","                            nn.Linear(state_dim, 64),\n","                            nn.Tanh(),\n","                            nn.Linear(64, 64),\n","                            nn.Tanh(),\n","                            nn.Linear(64, action_dim),\n","                            nn.Softmax(dim=-1)\n","                        )\n","        # defining the critic\n","        self.critic = nn.Sequential(\n","                        nn.Linear(state_dim, 64),\n","                        nn.Tanh(),\n","                        nn.Linear(64, 64),\n","                        nn.Tanh(),\n","                        nn.Linear(64, 1)\n","                    )\n","\n","    def set_action_std(self, new_action_std):\n","        #We only calculate the action std if we have a continuous action space\n","        if self.has_continuous_action_space:\n","            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n","        else:\n","            print(\"--------------------------------------------------------------------------------------------\")\n","            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n","            print(\"--------------------------------------------------------------------------------------------\")\n","\n","    def forward(self):\n","        raise NotImplementedError\n","\n","    def act(self, state):\n","        # if we have a continuous action space we sample from a multivariate normal distribution\n","        # otherwise we calculate a categorical action spac\n","        if self.has_continuous_action_space:\n","            action_mean = self.actor(state)\n","            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n","            dist = MultivariateNormal(action_mean, cov_mat)\n","        else:\n","            action_probs = self.actor(state)\n","            dist = Categorical(action_probs)\n","\n","        action = dist.sample()\n","        action_logprob = dist.log_prob(action)\n","\n","        return action.detach(), action_logprob.detach()\n","\n","    def evaluate(self, state, action):\n","\n","        if self.has_continuous_action_space:\n","            action_mean = self.actor(state)\n","\n","            action_var = self.action_var.expand_as(action_mean)\n","            cov_mat = torch.diag_embed(action_var).to(device)\n","            dist = MultivariateNormal(action_mean, cov_mat)\n","\n","            # For Single Action Environments.\n","            if self.action_dim == 1:\n","                action = action.reshape(-1, self.action_dim)\n","        else:\n","            action_probs = self.actor(state)\n","            dist = Categorical(action_probs)\n","        action_logprobs = dist.log_prob(action)\n","        dist_entropy = dist.entropy()\n","        state_values = self.critic(state)\n","\n","        return action_logprobs, state_values, dist_entropy\n"]},{"cell_type":"markdown","source":["##PPO"],"metadata":{"id":"eKo891_Luri6"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"ealjmWSObWdq","executionInfo":{"status":"ok","timestamp":1745690865396,"user_tz":-120,"elapsed":18,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"}}},"outputs":[],"source":["class PPO:\n","    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n","\n","        self.has_continuous_action_space = has_continuous_action_space\n","\n","        if has_continuous_action_space:\n","            self.action_std = action_std_init\n","\n","        self.gamma = gamma\n","        self.eps_clip = eps_clip\n","        self.K_epochs = K_epochs\n","\n","        self.buffer = RolloutBuffer()\n","\n","        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n","        self.optimizer = torch.optim.Adam([\n","                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n","                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n","                    ])\n","\n","        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","        self.MseLoss = nn.MSELoss()\n","\n","    def set_action_std(self, new_action_std):\n","        if self.has_continuous_action_space:\n","            self.action_std = new_action_std\n","            self.policy.set_action_std(new_action_std)\n","            self.policy_old.set_action_std(new_action_std)\n","        else:\n","            print(\"--------------------------------------------------------------------------------------------\")\n","            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n","            print(\"--------------------------------------------------------------------------------------------\")\n","\n","    def decay_action_std(self, action_std_decay_rate, min_action_std):\n","        print(\"--------------------------------------------------------------------------------------------\")\n","        if self.has_continuous_action_space:\n","            self.action_std = self.action_std - action_std_decay_rate\n","            self.action_std = round(self.action_std, 4)\n","            if (self.action_std <= min_action_std):\n","                self.action_std = min_action_std\n","                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n","            else:\n","                print(\"setting actor output action_std to : \", self.action_std)\n","            self.set_action_std(self.action_std)\n","\n","        else:\n","            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n","        print(\"--------------------------------------------------------------------------------------------\")\n","\n","    def select_action(self, state):\n","\n","        if self.has_continuous_action_space:\n","            with torch.no_grad():\n","                state = torch.FloatTensor(state).to(device)\n","                action, action_logprob = self.policy_old.act(state)\n","\n","            self.buffer.states.append(state.cpu())\n","            self.buffer.actions.append(action.cpu())\n","            self.buffer.logprobs.append(action_logprob.cpu())\n","\n","            return action.detach().cpu().numpy().flatten()\n","        else:\n","            with torch.no_grad():\n","                state = torch.FloatTensor(state).to(device)\n","                action, action_logprob = self.policy_old.act(state)\n","\n","            self.buffer.states.append(state)\n","            self.buffer.actions.append(action)\n","            self.buffer.logprobs.append(action_logprob)\n","\n","            return action.item()\n","\n","    def update(self):\n","        # Monte Carlo estimate of returns\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (self.gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","\n","        # Normalizing the rewards\n","        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","\n","        # convert list to tensor\n","        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n","        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n","        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n","\n","        # Optimize policy for K epochs\n","        for _ in range(self.K_epochs):\n","\n","            # Evaluating old actions and values\n","            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n","\n","            # match state_values tensor dimensions with rewards tensor\n","            state_values = torch.squeeze(state_values)\n","\n","            # Finding the ratio (pi_theta / pi_theta__old)\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","\n","            # Finding Surrogate Loss\n","            advantages = rewards - state_values.detach()\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","\n","            # final loss of clipped objective PPO\n","            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","\n","            # take gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","\n","        # Copy new weights into old policy\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","        # clear buffer\n","        self.buffer.clear()\n","\n","    def save(self, checkpoint_path):\n","        torch.save(self.policy_old.state_dict(), checkpoint_path)\n","\n","    def load(self, checkpoint_path):\n","        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n","        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"]},{"cell_type":"markdown","source":["##Helper Functions for the environments and rendering"],"metadata":{"id":"xDcMPok6uvNR"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"K5_5pBVNgfv_","executionInfo":{"status":"ok","timestamp":1745690869320,"user_tz":-120,"elapsed":5,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"}}},"outputs":[],"source":["#renders given frames with mediapy and shows a video\n","def renderEnv(frames):\n","  import mediapy as media\n","  media.show_video(frames,fps=60//4)\n","\n","#plot for visualizing results\n","def plotRewardandTime(avg_norm_reward,avg_length):\n","  import matplotlib.pyplot as plt\n","  x = np.linspace(0,len(avg_reward),len(avg_reward))\n","\n","  fig, axs = plt.subplots(1, 2,figsize=(9,3))\n","\n","  axs[0].plot(x, avg_norm_reward)\n","  axs[0].set_title(\"avg_norm_reward\")\n","\n","  axs[1].plot(x, avg_length)\n","  axs[1].set_title(\"avg_length\")\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Pk0QoPX5t3p"},"outputs":[],"source":["import gym\n","\n","#This environment wrapper is used to stop a run if mario is stuck on a pipe\n","class DeadlockEnv(gym.Wrapper):\n","    def __init__(self, env, threshold=10):\n","        super().__init__(env)\n","        self.last_x_pos = 0\n","        self.count = 0\n","        self.threshold = threshold\n","        self.lifes = 3\n","        self.stage = 1\n","        self.world = 1\n","\n","    def reset(self, **kwargs):\n","        self.last_x_pos = 0\n","        self.count = 0\n","        return self.env.reset(**kwargs)\n","\n","    def step(self, action):\n","        state, reward, done, info = self.env.step(action)\n","        x_pos = info['x_pos']\n","\n","        if x_pos <= self.last_x_pos:\n","            self.count += 1\n","        else:\n","            self.count = 0\n","            self.last_x_pos = x_pos\n","\n","        if info['life'] != self.lifes or info[\"stage\"] != self.stage or info[\"world\"] != self.world:\n","            self.last_x_pos = x_pos\n","            self.count = 0\n","            self.lifes = info['life']\n","            self.stage = info[\"stage\"]\n","            self.world = info[\"world\"]\n","\n","        if self.count >= self.threshold:\n","            reward = -15\n","            done = True\n","\n","        return state, reward, done, info\n","\n","#skipframe wrapper\n","class SkipFrame(gym.Wrapper):\n","    def __init__(self, env, skip):\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        reward_out = 0\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            reward_out += reward\n","            if done:\n","                break\n","        reward_out /= max(1,i+1)\n","\n","        return obs, reward_out, done, info\n","\n","#downsample wrapper to reduce dimensionality\n","def Downsample(ratio,state):\n","  (oldh, oldw, oldc) = state.shape\n","  newshape = (oldh//ratio, oldw//ratio, oldc)\n","  frame = cv2.resize(state, (newshape[0], newshape[1]), interpolation=cv2.INTER_AREA)\n","  return frame\n","\n","#small function to change rgb images to grayscale\n","def GrayScale(state):\n","  return cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n","\n"]},{"cell_type":"markdown","source":["#Training\n","\n","If you want to run this for more than an hour and really test how far your model gets, I would recommend to start a local runtime on your computer/laptop.\n","Here is a link on how to create a local runtime: https://research.google.com/colaboratory/local-runtimes.html"],"metadata":{"id":"aqeX3kGEyE45"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"4WeQxPlZcq4_","executionInfo":{"status":"error","timestamp":1745690888257,"user_tz":-120,"elapsed":386,"user":{"displayName":"Bjarne Thomzik","userId":"06928226671254719565"}},"colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"2d2e9b97-e58f-4fe3-8b1a-34df9ae4bac1"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'SkipFrame' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-956dbc28f34b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym_super_mario_bros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SuperMarioBros-v1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#the environment. v0 is with original background, v1 has the background removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJoypadSpace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIMPLE_MOVEMENT\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m#The Joypadspace sets the available actions. We use SIMPLE_MOVEMENT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkipFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m#Skipframewrapper to skip some frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeadlockEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m#Deadlock environment wrapper to stop the game if mario is stuck at a pipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'SkipFrame' is not defined"]}],"source":["from IPython.core.display import clear_output\n","\n","\n","state_dim = 15360                     # state space dimension\n","lr_actor = 0.0003                     # learning rate for actor network\n","lr_critic = 0.001                     # learning rate for critic network\n","gamma = 0.99                          # gamma discount\n","K_epochs = 50                         # K value for the PPO-CLIP objective function\n","eps_clip = 0.2                        # the epsilon clipping value\n","has_continuous_action_space = False   # the mario environment doesn't have a continuous action space\n","action_std = None                     # we don't change the action distribution\n","frameskip = 4                         # the frameskip value of the environment\n","down_sample_rate = 4                  # downsample rate. Calculated as: original_dimension/down_sample_rate\n","frame_stack = 4                       # frame stacking value\n","\n","env = gym_super_mario_bros.make('SuperMarioBros-v1')  #the environment. v0 is with original background, v1 has the background removed\n","env = JoypadSpace(env, SIMPLE_MOVEMENT)               #The Joypadspace sets the available actions. We use SIMPLE_MOVEMENT.\n","env = SkipFrame(env, skip=frameskip)                  #Skipframewrapper to skip some frames\n","env = DeadlockEnv(env,threshold=(60*2)//frameskip)                   #Deadlock environment wrapper to stop the game if mario is stuck at a pipe\n","\n","\n","\n","action_dim = env.action_space.n # action space dimension\n","#state_dim = env.state_space.n  # Currently we flatten the input and therefore set the state_dim manually\n","\n","ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n","\n","# track total training time\n","start_time = datetime.now().replace(microsecond=0)\n","print(\"Started training at (GMT) : \", start_time)\n","\n","print(\"============================================================================================\")\n","\n","#some helper variables\n","time_step = 0\n","max_training_epochs = 10000\n","max_ep_len = 10000\n","update_timestep = max_ep_len\n","\n","# We mount the google drive to save and load PPO states.\n","#from google.colab import drive\n","#drive.mount('/content/gdrive')\n","\n","# If an agent is saved you can uncomment the following line to load the weights.\n","#ppo_agent.load(\"/content/gdrive/My Drive/ppo.save\")\n","\n","avg_reward_temp = []\n","avg_length_temp = []\n","avg_norm_reward_temp = []\n","avg_reward = []\n","avg_length = []\n","avg_norm_reward = []\n","updates = 0\n","\n","episode_list = []\n","\n","tbar = tqdm(range(1,max_training_epochs))\n","for i in tbar:\n","    # first we reset the state\n","    state = env.reset()\n","    current_ep_reward = 0\n","    # as we stack some frames, we create a buffer with empty frames for the first inputs\n","    states_buffer = [np.zeros((3840,)) for _ in range(3)]\n","    frames = []\n","    # the collection loop\n","    for t in range(1, max_ep_len):\n","        # Downsampling the environment\n","        in_state = GrayScale(Downsample(down_sample_rate,state.copy())).flatten()\n","\n","        # creating the new stack for the current frame\n","        states_buffer.append(in_state/255)\n","        states_buffer = states_buffer[-frame_stack:]\n","\n","        # selecting an action\n","        action = ppo_agent.select_action(np.asarray(states_buffer).flatten())\n","        #print(np.asarray(states_buffer).flatten().min(),np.asarray(states_buffer).flatten().max())\n","\n","        # performing the action and receiving the information from the environments\n","        state, reward, done, info = env.step(action)\n","\n","        # Every 10 epochs we render the environments and therefore save the state\n","        if not done:\n","            frames.append(state.copy())\n","\n","        # The PPO agent needs the reward and the done state manually, as we could modify it.\n","        ppo_agent.buffer.rewards.append(reward)\n","        ppo_agent.buffer.is_terminals.append(done)\n","\n","        time_step +=1\n","        current_ep_reward += reward\n","\n","        # every update_steps (2048) we update the algorithm\n","        #if time_step % update_steps == 0:\n","\n","\n","        # if the run is done we break the loop\n","        if done:\n","            break\n","    if len(frames) > 0:\n","      episode_list.append((current_ep_reward,frames))\n","\n","\n","\n","    # We collect information every run and write them to the console\n","    avg_reward_temp.append(current_ep_reward)\n","    avg_length_temp.append(t)\n","    tbar.set_description(\"timestep: \" + str(time_step) + \" updates: \"+str(updates)+\" reward: \"+str(np.asarray(avg_norm_reward_temp[-50:]).mean()))\n","    avg_norm_reward_temp.append(current_ep_reward/max(1,t))\n","\n","    # Every 10 epochs we render the current environment\n","    if i % 10 == 0:\n","      ppo_agent.update()\n","      updates += 1\n","      avg_reward.append(np.median(avg_reward_temp))\n","      avg_length.append(np.median(avg_length_temp))\n","      avg_norm_reward.append(np.median(avg_norm_reward_temp))\n","\n","      if len(episode_list) > 0:\n","\n","        clear_output(wait=True)\n","        episode_list.sort(key=lambda x: x[0]) # we sort by the received reward and pick the best run to visualize\n","\n","        print(\"--------------------------------\")\n","        print(\"Epoch\",i,\"done:\")\n","        print(\"Update iterations:\",updates)\n","        print(\"Statistics:\")\n","        print(\"\")\n","        print(\"Reward of best episode:\",episode_list[-1][0])\n","        print(\"Length of best episode:\",len(episode_list[-1][1]))\n","        print(\"\")\n","        print(\"Average total reward:\",np.asarray(avg_reward[-50:]).mean())\n","        print(\"Average normalized reward:\",np.asarray(avg_norm_reward[-50:]).mean())\n","        print(\"Average length:\",np.asarray(avg_length[-50:]).mean())\n","        print(\"--------------------------------\")\n","        plotRewardandTime(avg_norm_reward,avg_length)\n","\n","        renderEnv(episode_list[-1][1])\n","        episode_list = []\n","\n","        #ppo_agent.save(\"/content/gdrive/My Drive/ppo.save\")\n","\n","\n","\n","\n","\n","\n","\n","env.close()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1DjgoJPE0IIPGf_t0f9Trgrpz4wRMPIJt","timestamp":1745688511642}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}