{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T12:26:22.203655Z",
     "start_time": "2025-06-25T12:26:22.198409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "print(Image.__file__)"
   ],
   "id": "3b785f3639031acf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lharms/.local/lib/python3.10/site-packages/PIL/Image.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163478/792723323.py:16: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Actor Critic",
   "id": "589a220893bcec17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T12:26:22.360783Z",
     "start_time": "2025-06-25T12:26:22.353006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # CNN-Feature-Extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # (4,84,84) → (32,20,20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # → (64,9,9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # → (64,7,7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()                                # → (64*7*7) = 3136\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def extract_features(self, state):\n",
    "        x = self.cnn(state)            # (B, 3136)\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        features = self.extract_features(state)\n",
    "        probs = self.actor(features)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.detach(), dist.log_prob(action).detach()\n",
    "\n",
    "    def evaluate(self, action, state):\n",
    "        features = self.extract_features(state)\n",
    "        probs = self.actor(features)\n",
    "        dist = Categorical(probs)\n",
    "        log_probs = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        value = self.critic(features)\n",
    "        return log_probs, entropy, value.squeeze(-1)\n"
   ],
   "id": "36f4d76db25036d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### A2C",
   "id": "a61f5ba726632df1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T12:26:22.432011Z",
     "start_time": "2025-06-25T12:26:22.422553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class A2C:\n",
    "    def __init__(self, action_dim, lr_actor, lr_critic, gamma):\n",
    "        self.policy = ActorCritic(action_dim).to(device ='cpu')\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device='cpu')\n",
    "        action, log_prob = self.policy.act(state)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "\n",
    "    def compute_returns(self, rewards, dones, last_value):\n",
    "        R = last_value\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            if done:\n",
    "                R = 0\n",
    "            R = reward + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def update(self, states, actions, log_probs, rewards, dones):\n",
    "        states = torch.FloatTensor(states).to(device = 'cpu')\n",
    "        actions = torch.LongTensor(actions).to(device = 'cpu')\n",
    "        old_log_probs = torch.stack(log_probs).to(device = 'cpu')\n",
    "\n",
    "        # Bootstrapping: Wert des letzten Zustands\n",
    "        with torch.no_grad():\n",
    "            last_state = torch.from_numpy(np.array(states[-1])).unsqueeze(0).to(dtype=torch.float32)\n",
    "            next_value = self.policy.critic(self.policy.cnn(last_state))\n",
    "\n",
    "        returns = self.compute_returns(rewards, dones, next_value.item())\n",
    "        returns = torch.FloatTensor(returns).to(device = 'cpu')\n",
    "\n",
    "        # Evaluieren der aktuellen Policy\n",
    "        log_probs_new, entropy, values = self.policy.evaluate( actions, states)\n",
    "\n",
    "        advantages = returns - values.detach()\n",
    "\n",
    "        actor_loss = - (log_probs_new * advantages).mean()\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "\n",
    "        loss = actor_loss + 0.5 * critic_loss - 0.05 * entropy.mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ],
   "id": "3d1b58e3c4238b9e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### environment",
   "id": "8b6f1ad7a440009c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-25T12:26:22.498481Z",
     "start_time": "2025-06-25T12:26:22.490211Z"
    }
   },
   "source": [
    "import gym\n",
    "\n",
    "#This environment wrapper is used to stop a run if mario is stuck on a pipe\n",
    "class DeadlockEnv(gym.Wrapper):\n",
    "    def __init__(self, env, threshold=10):\n",
    "        super().__init__(env)\n",
    "        self.last_x_pos = 0\n",
    "        self.count = 0\n",
    "        self.threshold = threshold\n",
    "        self.lifes = 3\n",
    "        self.stage = 1\n",
    "        self.world = 1\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.last_x_pos = 0\n",
    "        self.count = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        x_pos = info['x_pos']\n",
    "\n",
    "        if x_pos <= self.last_x_pos:\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.count = 0\n",
    "            self.last_x_pos = x_pos\n",
    "\n",
    "        if info['life'] != self.lifes or info[\"stage\"] != self.stage or info[\"world\"] != self.world:\n",
    "            self.last_x_pos = x_pos\n",
    "            self.count = 0\n",
    "            self.lifes = info['life']\n",
    "            self.stage = info[\"stage\"]\n",
    "            self.world = info[\"world\"]\n",
    "\n",
    "        if self.count >= self.threshold:\n",
    "            reward = -15\n",
    "            done = True\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "#skipframe wrapper\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        reward_out = 0\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            reward_out += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward_out /= max(1,i+1)\n",
    "\n",
    "        return obs, reward_out, done, info\n",
    "\n",
    "#downsample wrapper to reduce dimensionality\n",
    "def Downsample(ratio,state):\n",
    "  (oldh, oldw, oldc) = state.shape\n",
    "  newshape = (oldh//ratio, oldw//ratio, oldc)\n",
    "  frame = cv2.resize(state, (newshape[0], newshape[1]), interpolation=cv2.INTER_AREA)\n",
    "  return frame\n",
    "\n",
    "#small function to change rgb images to grayscale\n",
    "def GrayScale(state):\n",
    "  return cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training",
   "id": "f322485ea5bc1c3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T12:26:22.561357Z",
     "start_time": "2025-06-25T12:26:22.557506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#renders given frames with mediapy and shows a video\n",
    "def renderEnv(frames):\n",
    "  import mediapy as media\n",
    "  media.show_video(frames,fps=60//4)\n",
    "\n",
    "#plot for visualizing results\n",
    "def plotRewardandTime(avg_norm_reward,avg_length):\n",
    "  import matplotlib.pyplot as plt\n",
    "  x = np.arange(len(avg_norm_reward))\n",
    "\n",
    "  fig, axs = plt.subplots(1, 2,figsize=(9,3))\n",
    "\n",
    "  axs[0].plot(x, avg_norm_reward)\n",
    "  axs[0].set_title(\"avg_norm_reward\")\n",
    "\n",
    "  axs[1].plot(x, avg_length)\n",
    "  axs[1].set_title(\"avg_length\")\n",
    "  plt.show()"
   ],
   "id": "e9c72ff7848509f2",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T12:26:24.448091Z",
     "start_time": "2025-06-25T12:26:22.626583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frameskip = 4\n",
    "down_sample_rate = 4\n",
    "lr_actor = 0.0003\n",
    "lr_critic = 0.001\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v1')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = SkipFrame(env, skip=frameskip)                  #Skipframewrapper to skip some frames\n",
    "env = DeadlockEnv(env,threshold=(60*2)//frameskip)\n",
    "state = env.reset()\n",
    "\n",
    "action_dim = env.action_space.n # action space dimension\n",
    "\n",
    "agent = A2C(action_dim=action_dim, lr_actor=lr_actor, lr_critic=lr_critic, gamma=gamma)\n",
    "\n",
    "done = False\n",
    "log_probs, states, actions, rewards, dones = [], [], [], [], []\n",
    "\n",
    "frame_stack = 4\n",
    "states_buffer = [np.zeros((84, 84), dtype=np.float32) for _ in range(frame_stack - 1)]\n",
    "for i, f in enumerate(states_buffer):\n",
    "    if f.shape != (84, 84):\n",
    "        print(f\"Invalid shape at index {i}: {f.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "avg_reward_temp, avg_length_temp, avg_norm_reward_temp = [], [], []\n",
    "avg_reward, avg_length, avg_norm_reward = [], [], []\n",
    "\n",
    "max_training_epochs = 1000\n",
    "max_ep_len = 100000\n",
    "updates = 0\n",
    "episode_list = []\n",
    "\n",
    "for epoch in range(max_training_epochs):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    states, actions, log_probs, rewards, dones = [], [], [], [], []\n",
    "\n",
    "    frames = []  # Optional: Für Visualisierung\n",
    "\n",
    "    for t in range(max_ep_len):\n",
    "        env.render()\n",
    "\n",
    "        # Bild vorbereiten\n",
    "        # Statt flatten() → 84x84 behalten\n",
    "        in_state = GrayScale(Downsample(down_sample_rate, state.copy()))  # shape: (84, 84)\n",
    "        in_state = cv2.resize(in_state, (84, 84))\n",
    "        states_buffer.append(in_state / 255.0)\n",
    "        states_buffer = states_buffer[-frame_stack:]\n",
    "\n",
    "        \"\"\"\n",
    "        in_state = GrayScale(Downsample(down_sample_rate, state.copy())).flatten()\n",
    "        states_buffer.append(in_state / 255.0)\n",
    "        states_buffer = states_buffer[-frame_stack:]\n",
    "        \"\"\"\n",
    "\n",
    "        # Aktion auswählen\n",
    "        stacked_input = np.stack(states_buffer, axis=0)  # shape: (4, 84, 84)\n",
    "        action, log_prob = agent.select_action(stacked_input)\n",
    "\n",
    "        # Schritt durchführen\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Speichern\n",
    "        states.append(stacked_input)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if not done:\n",
    "            frames.append(state.copy())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_reward_temp.append(episode_reward)\n",
    "    avg_length_temp.append(t)\n",
    "    avg_norm_reward_temp.append(episode_reward / max(1, t))\n",
    "\n",
    "\n",
    "    if len(frames) > 0:\n",
    "        episode_list.append((episode_reward, frames))\n",
    "    else:\n",
    "        episode_list.append((episode_reward, []))  # oder ggf. letzte Frames speichern\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        agent.update(states, actions, log_probs, rewards, dones)\n",
    "        updates += 1\n",
    "\n",
    "        avg_reward.append(np.median(avg_reward_temp))\n",
    "        avg_length.append(np.median(avg_length_temp))\n",
    "        avg_norm_reward.append(np.median(avg_norm_reward_temp))\n",
    "\n",
    "        avg_reward_temp.clear()\n",
    "        avg_length_temp.clear()\n",
    "        avg_norm_reward_temp.clear()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        episode_list.sort(key=lambda x: x[0])  # Sortiere nach Reward\n",
    "\n",
    "        print(\"Updating at epoch\", epoch)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Epoch\", epoch, \"done:\")\n",
    "        print(\"Update iterations:\", updates)\n",
    "        print(\"Statistics:\")\n",
    "        print(\"\")\n",
    "        print(\"Reward of best episode:\", episode_list[-1][0])\n",
    "        print(\"Length of best episode:\", len(episode_list[-1][1]))\n",
    "        print(\"\")\n",
    "        print(\"Average total reward:\", np.mean(avg_reward[-50:]))\n",
    "        print(\"Average normalized reward:\", np.mean(avg_norm_reward[-50:]))\n",
    "        print(\"Average length:\", np.mean(avg_length[-50:]))\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "        plotRewardandTime(avg_norm_reward, avg_length)\n",
    "        renderEnv(episode_list[-1][1])\n",
    "\n",
    "        # Nur alte Episoden löschen, falls Liste zu lang\n",
    "        if len(episode_list) > 10:\n",
    "            episode_list = episode_list[-10:]\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} | Reward: {episode_reward:.2f} | Avg Reward (last 50): {np.mean(avg_reward[-50:]):.2f}\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save(agent.policy.state_dict(), f\"../runs/A2C/testCNN/07-inc_ep_100000/a2c_epoch_{epoch}.pt\")\n",
    "\n",
    "env.close()"
   ],
   "id": "fe6e896e322b5d13",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A2C.update() takes 2 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 93\u001B[0m\n\u001B[1;32m     90\u001B[0m     episode_list\u001B[38;5;241m.\u001B[39mappend((episode_reward, []))  \u001B[38;5;66;03m# oder ggf. letzte Frames speichern\u001B[39;00m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 93\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_probs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrewards\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdones\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m     updates \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     96\u001B[0m     avg_reward\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mmedian(avg_reward_temp))\n",
      "\u001B[0;31mTypeError\u001B[0m: A2C.update() takes 2 positional arguments but 6 were given"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
